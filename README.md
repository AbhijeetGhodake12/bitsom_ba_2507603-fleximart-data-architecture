# FlexiMart Data Architecture Project

**Student Name:** Abhijeet Ghodake  
**Student ID:** bitsom_ba_2507603  
**Email:** ghodake.abhijeet1999@gmail.com  
**Date:** 8/1/26

## Project Overview

This project implements a comprehensive data architecture solution for FlexiMart, an e-commerce platform. The solution includes a relational database ETL pipeline for data quality management, a NoSQL implementation for flexible product catalog management, and a data warehouse with star schema design for business intelligence and analytics. The project demonstrates end-to-end data engineering practices from extraction and transformation to analytical reporting.

## Repository Structure

```
studentID-fleximart-data-architecture/
│
├── README.md                           # Root documentation
├── .gitignore                          # Ignore unnecessary files
│
├── data/                               # Input data files (provided)
│   ├── customers_raw.csv
│   ├── products_raw.csv
│   └── sales_raw.csv
│
├── part1-database-etl/
│   ├── README.md                       # Part 1 overview
│   ├── etl_pipeline.py
│   ├── schema_documentation.md
│   ├── business_queries.sql
│   ├── data_quality_report.txt         # Generated by ETL script
│   └── requirements.txt                # Python dependencies
│
├── part2-nosql/
│   ├── README.md                       # Part 2 overview
│   ├── nosql_analysis.md
│   ├── mongodb_operations.js
│   └── products_catalog.json
│
└── part3-datawarehouse/
    ├── README.md                       # Part 3 overview
    ├── star_schema_design.md
    ├── warehouse_schema.sql
    ├── warehouse_data.sql
    └── analytics_queries.sql
```

## Technologies Used

- **Python 3.x**: Core programming language for ETL pipeline
- **pandas**: Data manipulation and transformation
- **mysql-connector-python**: MySQL database connectivity
- **MySQL 8.0**: Relational database management system
- **MongoDB 6.0**: NoSQL document database
- **SQL**: Query language for relational databases and data warehouse

## Setup Instructions

### Prerequisites

1. Install Python 3.x
2. Install MySQL 8.0 or PostgreSQL 14
3. Install MongoDB 6.0
4. Install required Python packages

### Database Setup

#### MySQL Setup

```bash
# Create operational database
mysql -u root -p -e "CREATE DATABASE fleximart;"

# Create data warehouse database
mysql -u root -p -e "CREATE DATABASE fleximart_dw;"
```

#### Part 1 - ETL Pipeline and Relational Database

```bash
# Navigate to part1-database-etl directory
cd part1-database-etl

# Install Python dependencies
pip install -r requirements.txt

# Update database credentials in etl_pipeline.py
# Run ETL pipeline (without database loading)
python etl_pipeline.py

# To load data into MySQL, set load_to_db=True in main() function
# Or run business queries directly
mysql -u root -p fleximart < business_queries.sql
```

#### Part 2 - NoSQL Implementation

```bash
# Navigate to part2-nosql directory
cd part2-nosql

# Import products catalog into MongoDB
mongoimport --db fleximart --collection products --file products_catalog.json --jsonArray

# Run MongoDB operations
mongosh < mongodb_operations.js

# Or connect to MongoDB shell and run operations manually
mongosh
use fleximart
# Then execute queries from mongodb_operations.js
```

#### Part 3 - Data Warehouse

```bash
# Navigate to part3-datawarehouse directory
cd part3-datawarehouse

# Create warehouse schema
mysql -u root -p fleximart_dw < warehouse_schema.sql

# Load warehouse data
mysql -u root -p fleximart_dw < warehouse_data.sql

# Run analytics queries
mysql -u root -p fleximart_dw < analytics_queries.sql
```

### Configuration

1. **MySQL Configuration**: Update database credentials in `part1-database-etl/etl_pipeline.py`
   ```python
   db_config = {
       'host': 'localhost',
       'user': 'root',
       'password': 'your_password',
       'database': 'fleximart'
   }
   ```

2. **MongoDB Configuration**: Ensure MongoDB is running on default port 27017

## Project Components

### Part 1: Database ETL Pipeline

- **etl_pipeline.py**: Complete ETL pipeline with Extract, Transform, and Load phases
- **schema_documentation.md**: Database schema documentation with entity descriptions and normalization explanation
- **business_queries.sql**: SQL queries for customer purchase history, product sales analysis, and monthly sales trends

**Key Features:**
- Data quality improvements (duplicate removal, missing value handling, standardization)
- Phone number and date format standardization
- Surrogate key generation
- MySQL database integration

### Part 2: NoSQL Implementation

- **nosql_analysis.md**: Analysis of RDBMS limitations and NoSQL benefits
- **mongodb_operations.js**: MongoDB operations for product catalog management
- **products_catalog.json**: Sample product data with nested reviews

**Key Features:**
- Flexible document schema for varying product attributes
- Embedded documents for customer reviews
- Product catalog operations (queries, aggregations, updates)

### Part 3: Data Warehouse

- **star_schema_design.md**: Star schema design documentation
- **warehouse_schema.sql**: Data warehouse table definitions
- **warehouse_data.sql**: Sample data for dimensions and fact table
- **analytics_queries.sql**: Analytical queries for business intelligence

**Key Features:**
- Star schema design with fact and dimension tables
- Time-based analysis capabilities
- Product performance analysis
- Customer segmentation

## Key Learnings

This project provided comprehensive hands-on experience with modern data architecture patterns. I learned how to design and implement ETL pipelines that handle real-world data quality issues, including duplicate detection, missing value imputation, and data standardization. The NoSQL component demonstrated the flexibility of document databases for managing heterogeneous data structures, particularly useful for product catalogs with varying attributes. The data warehouse implementation taught me star schema design principles, surrogate key management, and how to write analytical queries that support business decision-making through drill-down and roll-up operations.

## Challenges Faced

1. **Data Quality Issues**: Handling inconsistent phone formats, date formats, and missing values required careful transformation logic. Solution: Implemented standardization functions for phone numbers (converting various formats to +91-XXXXXXXXXX) and dates (normalizing to YYYY-MM-DD), and used appropriate strategies for missing data (filling with defaults, medians, or dropping records based on business rules).

2. **Schema Evolution**: The transition from relational to NoSQL required rethinking data modeling approaches. Solution: Analyzed the limitations of rigid schemas for diverse product types and implemented MongoDB's flexible document structure, allowing each product to have different attributes without schema migrations.

3. **Star Schema Design**: Creating an effective star schema that supports both detailed and aggregated analysis was challenging. Solution: Designed the schema with transaction line-item granularity, implemented surrogate keys for stability, and created comprehensive date and dimension tables that enable flexible time-based and multi-dimensional analysis.

4. **Complex Aggregations**: Writing efficient analytical queries with percentage calculations and customer segmentation required advanced SQL techniques. Solution: Used subqueries for percentage calculations, implemented CASE statements for customer segmentation, and leveraged window functions where appropriate for better performance and readability.

## File Descriptions

### Part 1 - Database ETL
- `etl_pipeline.py`: Main ETL pipeline implementation
- `schema_documentation.md`: Complete database schema documentation
- `business_queries.sql`: Business intelligence queries
- `requirements.txt`: Python dependencies

### Part 2 - NoSQL
- `nosql_analysis.md`: NoSQL justification and analysis
- `mongodb_operations.js`: MongoDB operations and queries
- `products_catalog.json`: Sample product catalog data

### Part 3 - Data Warehouse
- `star_schema_design.md`: Star schema design documentation
- `warehouse_schema.sql`: Data warehouse table definitions
- `warehouse_data.sql`: Sample warehouse data
- `analytics_queries.sql`: Analytical queries for BI

## Testing

To verify the implementation:

1. **ETL Pipeline**: Check cleaned CSV files in `part1-database-etl/` directory
2. **MySQL Database**: Query tables to verify data integrity
3. **MongoDB**: Run queries from `mongodb_operations.js` to verify operations
4. **Data Warehouse**: Execute analytics queries to verify star schema functionality

## Notes

- Ensure all databases are running before executing scripts
- Update database credentials in configuration files
- MongoDB operations require MongoDB shell (mongosh) or Node.js driver
- All SQL files are tested and ready for execution
